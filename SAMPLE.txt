# ========== Standard Library ==========
import os
import sqlite3
import asyncio
import threading
import tempfile
from datetime import datetime
from typing import TypedDict, Annotated, Dict, Any, Optional

# ========== Third-Party Libraries ==========
import aiohttp
import aiofiles
import aiosqlite
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import requests   # For stock price tool

# ========== LangChain / LangGraph ==========
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.tools import tool, BaseTool
from langchain_core.runnables import RunnableConfig
from langchain_community.tools import DuckDuckGoSearchRun

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

# PDF + Text Split Tools
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# ========== MCP Client ==========
from langchain_mcp_adapters.client import MultiServerMCPClient

# Environment
load_dotenv()


# ---------------------------------------
# DEDICATED ASYNC LOOP (REQUIRED)
# ---------------------------------------
_ASYNC_LOOP = asyncio.new_event_loop()

def _loop_runner():
    asyncio.set_event_loop(_ASYNC_LOOP)
    _ASYNC_LOOP.run_forever()

_ASYNC_THREAD = threading.Thread(target=_loop_runner, daemon=True)
_ASYNC_THREAD.start()


def _submit_async(coro):
    return asyncio.run_coroutine_threadsafe(coro, _ASYNC_LOOP)


def run_async(coro):
    return _submit_async(coro).result()


def submit_async_task(coro):
    """Schedule a coroutine on the backend event loop."""
    return _submit_async(coro)


# ---------------------------------------
# LLM + EMBEDDINGS
# ---------------------------------------
llm = ChatOllama(model="llama3.2:3b")
embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")


# ---------------------------------------
# PDF RETRIEVER PER THREAD
# ---------------------------------------
_THREAD_RETRIEVERS: Dict[str, Any] = {}
_THREAD_METADATA: Dict[str, dict] = {}


def _get_retriever(thread_id: Optional[str]):
    """Fetch the retriever for a thread if available."""
    return _THREAD_RETRIEVERS.get(thread_id)


def ingest_pdf(file_bytes: bytes, thread_id: str, filename: Optional[str] = None) -> dict:
    """Build a FAISS retriever from uploaded PDF and store it per thread."""
    if not file_bytes:
        raise ValueError("No bytes received for ingestion.")

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
        temp_file.write(file_bytes)
        temp_path = temp_file.name

    try:
        loader = PyPDFLoader(temp_path)
        docs = loader.load()

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
        chunks = splitter.split_documents(docs)

        vector_store = FAISS.from_documents(chunks, embeddings)
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}
        )

        _THREAD_RETRIEVERS[str(thread_id)] = retriever
        _THREAD_METADATA[str(thread_id)] = {
            "filename": filename or os.path.basename(temp_path),
            "documents": len(docs),
            "chunks": len(chunks),
        }

        return _THREAD_METADATA[str(thread_id)]

    finally:
        try:
            os.remove(temp_path)
        except OSError:
            pass


# ---------------------------------------
# TOOLS
# ---------------------------------------
search_tool = DuckDuckGoSearchRun(region="us-en")


@tool
def get_stock_price(symbol: str) -> dict:
    """Fetch latest stock price using Alpha Vantage."""
    url = f"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={symbol}&apikey=C9PE94QUEW9VWGFM"
    r = requests.get(url)
    return r.json()


# ---------------------------------------
# MCP TOOLING
# ---------------------------------------
client = MultiServerMCPClient(
    {
        "arith": {
            "transport": "stdio",
            "command": "python",
            "args": [
                r"C:\Users\LOQ\Downloads\CHATBOT_USING_LANGGRAPH\MCPSERVER\server.py"
            ],
        },
        "expense": {
            "transport": "streamable_http",
            "url": "https://considerable-lime-toad.fastmcp.app/mcp"
        },
    }
)


def load_mcp_tools() -> list:
    try:
        return run_async(client.get_tools())
    except Exception:
        return []


mcp_tools = load_mcp_tools()

tools = [search_tool, get_stock_price, *mcp_tools]
llm_with_tools = llm.bind_tools(tools) if tools else llm


# ---------------------------------------
# STATE
# ---------------------------------------
class ChatState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]


# ---------------------------------------
# GRAPH NODES
# ---------------------------------------
async def chat_node(state: ChatState, config: RunnableConfig = None):
    """LLM node that may answer or call a tool, with optional RAG."""
    messages = state["messages"]
    
    # Check if this thread has a PDF retriever
    thread_id = None
    if config and "configurable" in config:
        thread_id = config["configurable"].get("thread_id")
    
    retriever = _get_retriever(thread_id) if thread_id else None
    
    # If retriever exists and last message is from user, augment with context
    if retriever and messages and isinstance(messages[-1], HumanMessage):
        query = messages[-1].content
        try:
            # Retrieve relevant chunks
            docs = retriever.get_relevant_documents(query)
            if docs:
                context = "\n\n".join([doc.page_content for doc in docs])
                augmented_query = f"""Context from uploaded PDF:
{context}

User question: {query}

Please answer based on the provided context if relevant."""
                # Replace the last message with augmented version
                messages = messages[:-1] + [HumanMessage(content=augmented_query)]
        except Exception as e:
            # If retrieval fails, continue without augmentation
            print(f"Retrieval error: {e}")
    
    response = await llm_with_tools.ainvoke(messages)
    return {"messages": [response]}


tool_node = ToolNode(tools) if tools else None


# ---------------------------------------
# CHECKPOINTER
# ---------------------------------------
async def _init_checkpointer():
    conn = await aiosqlite.connect(database="chatbot.db")
    return AsyncSqliteSaver(conn)

checkpointer = run_async(_init_checkpointer())


# ---------------------------------------
# GRAPH BUILD
# ---------------------------------------
graph = StateGraph(ChatState)

graph.add_node("chat_node", chat_node)
graph.add_edge(START, "chat_node")

if tool_node:
    graph.add_node("tools", tool_node)
    graph.add_conditional_edges("chat_node", tools_condition)
    graph.add_edge("tools", "chat_node")
else:
    graph.add_edge("chat_node", END)

# Compile with checkpointer
chatbot = graph.compile(checkpointer=checkpointer)


# ---------------------------------------
# THREAD LISTING
# ---------------------------------------
async def _alist_threads():
    threads = set()
    async for cp in checkpointer.alist(None):
        threads.add(cp.config["configurable"]["thread_id"])
    return list(threads)


def retrieve_all_threads():
    return run_async(_alist_threads())
